{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torchvision\n",
    "import torchvision.datasets as data\n",
    "import torchvision.transforms as trans\n",
    "from torch.utils.tensorboard import SummaryWriter as summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import backward\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DIRECTORIES USED IN THIS PROGRAM HERE\n",
    "\n",
    "if not os.path.exists(\"data/MNIST\"):\n",
    "    os.makedirs(\"data/MNIST\")\n",
    "\n",
    "if not os.path.exists(\"data/GAN/saved\"):\n",
    "    os.makedirs(\"data/GAN/saved\")\n",
    "\n",
    "if not os.path.exists(\"data/GAN/logs\"):\n",
    "    os.makedirs(\"data/GAN/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to enable CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Latent space dimension\n",
    "z = 64\n",
    "\n",
    "# Image size (MNIST images are 28*28 pixels = 784)\n",
    "image_size = 784\n",
    "\n",
    "# Number of images in batch\n",
    "batch_size = 64\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 250\n",
    "\n",
    "# Directory configuration\n",
    "dataset_dir  = \"data/MNIST\"\n",
    "output_dir   = \"data/GAN/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator -- distinguish images from ones it has seen already\n",
    "# Currently: 2 layers, ReLU (mx = 0.1) and sigmoidal (output [0,1]) activation functions \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.d = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.d(x)\n",
    "    \n",
    "# Generator -- produce new images to try and fool the discriminator\n",
    "# Currently: 2 layers, ReLU (mx = 0.1) and horizontal tangent ()\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z, image_size):\n",
    "        super().__init__()\n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(z, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, image_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator(image_size).to(device)\n",
    "g = Generator(z, image_size).to(device)\n",
    "\n",
    "\n",
    "set_noise = torch.randn((batch_size, z)).to(device)\n",
    "\n",
    "transform = trans.Compose([trans.ToTensor(),trans.Normalize((0.5),(0.5))])\n",
    "\n",
    "dataset   = data.MNIST(root='data/MNIST/', transform=transform, download=True)\n",
    "loader    = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "d_optim   = opt.Adam(d.parameters(), lr=learning_rate)\n",
    "g_optim   = opt.Adam(g.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Save real and generated images \n",
    "fake_imgs = summary(f\"data/GAN/logs/fake\")\n",
    "real_imgs = summary(f\"data/GAN/logs/real\")\n",
    "\n",
    "step_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: [0/250]; Batch 0/938; Discriminator Loss: 0.3876, Generator Loss: 1.8323\n",
      "Epoch Number: [1/250]; Batch 0/938; Discriminator Loss: 0.3044, Generator Loss: 2.0361\n",
      "Epoch Number: [2/250]; Batch 0/938; Discriminator Loss: 0.3396, Generator Loss: 1.9707\n",
      "Epoch Number: [3/250]; Batch 0/938; Discriminator Loss: 0.2968, Generator Loss: 1.9761\n",
      "Epoch Number: [4/250]; Batch 0/938; Discriminator Loss: 0.2596, Generator Loss: 2.1557\n",
      "Epoch Number: [5/250]; Batch 0/938; Discriminator Loss: 0.3453, Generator Loss: 2.1991\n",
      "Epoch Number: [6/250]; Batch 0/938; Discriminator Loss: 0.3818, Generator Loss: 1.7871\n",
      "Epoch Number: [7/250]; Batch 0/938; Discriminator Loss: 0.4309, Generator Loss: 1.8748\n",
      "Epoch Number: [8/250]; Batch 0/938; Discriminator Loss: 0.5310, Generator Loss: 1.9068\n",
      "Epoch Number: [9/250]; Batch 0/938; Discriminator Loss: 0.5965, Generator Loss: 1.6049\n",
      "Epoch Number: [10/250]; Batch 0/938; Discriminator Loss: 0.6560, Generator Loss: 1.5232\n",
      "Epoch Number: [11/250]; Batch 0/938; Discriminator Loss: 0.3947, Generator Loss: 2.0181\n",
      "Epoch Number: [12/250]; Batch 0/938; Discriminator Loss: 0.4822, Generator Loss: 1.6698\n",
      "Epoch Number: [13/250]; Batch 0/938; Discriminator Loss: 0.6329, Generator Loss: 1.4665\n",
      "Epoch Number: [14/250]; Batch 0/938; Discriminator Loss: 0.6803, Generator Loss: 1.1307\n",
      "Epoch Number: [15/250]; Batch 0/938; Discriminator Loss: 0.6248, Generator Loss: 1.5559\n",
      "Epoch Number: [16/250]; Batch 0/938; Discriminator Loss: 0.4907, Generator Loss: 1.5556\n",
      "Epoch Number: [17/250]; Batch 0/938; Discriminator Loss: 0.4220, Generator Loss: 1.7690\n",
      "Epoch Number: [18/250]; Batch 0/938; Discriminator Loss: 0.4401, Generator Loss: 1.8378\n",
      "Epoch Number: [19/250]; Batch 0/938; Discriminator Loss: 0.4472, Generator Loss: 1.6127\n",
      "Epoch Number: [20/250]; Batch 0/938; Discriminator Loss: 0.4366, Generator Loss: 1.9503\n",
      "Epoch Number: [21/250]; Batch 0/938; Discriminator Loss: 0.4630, Generator Loss: 1.6264\n",
      "Epoch Number: [22/250]; Batch 0/938; Discriminator Loss: 0.4994, Generator Loss: 1.5769\n",
      "Epoch Number: [23/250]; Batch 0/938; Discriminator Loss: 0.6475, Generator Loss: 1.5233\n",
      "Epoch Number: [24/250]; Batch 0/938; Discriminator Loss: 0.6352, Generator Loss: 1.4079\n",
      "Epoch Number: [25/250]; Batch 0/938; Discriminator Loss: 0.4065, Generator Loss: 1.8551\n",
      "Epoch Number: [26/250]; Batch 0/938; Discriminator Loss: 0.4956, Generator Loss: 1.6211\n",
      "Epoch Number: [27/250]; Batch 0/938; Discriminator Loss: 0.5748, Generator Loss: 1.5745\n",
      "Epoch Number: [28/250]; Batch 0/938; Discriminator Loss: 0.5522, Generator Loss: 1.5330\n",
      "Epoch Number: [29/250]; Batch 0/938; Discriminator Loss: 0.5217, Generator Loss: 1.3127\n",
      "Epoch Number: [30/250]; Batch 0/938; Discriminator Loss: 0.3256, Generator Loss: 1.8789\n",
      "Epoch Number: [31/250]; Batch 0/938; Discriminator Loss: 0.5524, Generator Loss: 1.5773\n",
      "Epoch Number: [32/250]; Batch 0/938; Discriminator Loss: 0.5713, Generator Loss: 1.4864\n",
      "Epoch Number: [33/250]; Batch 0/938; Discriminator Loss: 0.6378, Generator Loss: 1.4331\n",
      "Epoch Number: [34/250]; Batch 0/938; Discriminator Loss: 0.5667, Generator Loss: 1.1996\n",
      "Epoch Number: [35/250]; Batch 0/938; Discriminator Loss: 0.5301, Generator Loss: 1.2286\n",
      "Epoch Number: [36/250]; Batch 0/938; Discriminator Loss: 0.5345, Generator Loss: 1.2897\n",
      "Epoch Number: [37/250]; Batch 0/938; Discriminator Loss: 0.5015, Generator Loss: 1.3762\n",
      "Epoch Number: [38/250]; Batch 0/938; Discriminator Loss: 0.6542, Generator Loss: 1.2345\n",
      "Epoch Number: [39/250]; Batch 0/938; Discriminator Loss: 0.3648, Generator Loss: 1.6463\n",
      "Epoch Number: [40/250]; Batch 0/938; Discriminator Loss: 0.4760, Generator Loss: 1.4757\n",
      "Epoch Number: [41/250]; Batch 0/938; Discriminator Loss: 0.5249, Generator Loss: 1.6275\n",
      "Epoch Number: [42/250]; Batch 0/938; Discriminator Loss: 0.4500, Generator Loss: 1.7090\n",
      "Epoch Number: [43/250]; Batch 0/938; Discriminator Loss: 0.3694, Generator Loss: 1.6786\n",
      "Epoch Number: [44/250]; Batch 0/938; Discriminator Loss: 0.6779, Generator Loss: 1.1404\n",
      "Epoch Number: [45/250]; Batch 0/938; Discriminator Loss: 0.5213, Generator Loss: 1.5883\n",
      "Epoch Number: [46/250]; Batch 0/938; Discriminator Loss: 0.6992, Generator Loss: 1.0705\n",
      "Epoch Number: [47/250]; Batch 0/938; Discriminator Loss: 0.5436, Generator Loss: 1.3774\n",
      "Epoch Number: [48/250]; Batch 0/938; Discriminator Loss: 0.5900, Generator Loss: 1.4494\n",
      "Epoch Number: [49/250]; Batch 0/938; Discriminator Loss: 0.5010, Generator Loss: 1.4774\n",
      "Epoch Number: [50/250]; Batch 0/938; Discriminator Loss: 0.6648, Generator Loss: 1.6421\n",
      "Epoch Number: [51/250]; Batch 0/938; Discriminator Loss: 0.7784, Generator Loss: 1.0769\n",
      "Epoch Number: [52/250]; Batch 0/938; Discriminator Loss: 0.6531, Generator Loss: 1.1201\n",
      "Epoch Number: [53/250]; Batch 0/938; Discriminator Loss: 0.6710, Generator Loss: 1.4128\n",
      "Epoch Number: [54/250]; Batch 0/938; Discriminator Loss: 0.6199, Generator Loss: 1.0713\n",
      "Epoch Number: [55/250]; Batch 0/938; Discriminator Loss: 0.5896, Generator Loss: 1.2836\n",
      "Epoch Number: [56/250]; Batch 0/938; Discriminator Loss: 0.6105, Generator Loss: 1.1776\n",
      "Epoch Number: [57/250]; Batch 0/938; Discriminator Loss: 0.6383, Generator Loss: 1.2415\n",
      "Epoch Number: [58/250]; Batch 0/938; Discriminator Loss: 0.5295, Generator Loss: 1.4014\n",
      "Epoch Number: [59/250]; Batch 0/938; Discriminator Loss: 0.5871, Generator Loss: 1.1566\n",
      "Epoch Number: [60/250]; Batch 0/938; Discriminator Loss: 0.5743, Generator Loss: 1.2716\n",
      "Epoch Number: [61/250]; Batch 0/938; Discriminator Loss: 0.5088, Generator Loss: 1.3857\n",
      "Epoch Number: [62/250]; Batch 0/938; Discriminator Loss: 0.5279, Generator Loss: 1.2953\n",
      "Epoch Number: [63/250]; Batch 0/938; Discriminator Loss: 0.6013, Generator Loss: 1.3820\n",
      "Epoch Number: [64/250]; Batch 0/938; Discriminator Loss: 0.5568, Generator Loss: 1.5520\n",
      "Epoch Number: [65/250]; Batch 0/938; Discriminator Loss: 0.4987, Generator Loss: 1.4822\n",
      "Epoch Number: [66/250]; Batch 0/938; Discriminator Loss: 0.5406, Generator Loss: 1.2316\n",
      "Epoch Number: [67/250]; Batch 0/938; Discriminator Loss: 0.5950, Generator Loss: 1.1491\n",
      "Epoch Number: [68/250]; Batch 0/938; Discriminator Loss: 0.5772, Generator Loss: 1.1715\n",
      "Epoch Number: [69/250]; Batch 0/938; Discriminator Loss: 0.5946, Generator Loss: 1.2754\n",
      "Epoch Number: [70/250]; Batch 0/938; Discriminator Loss: 0.5000, Generator Loss: 1.2324\n",
      "Epoch Number: [71/250]; Batch 0/938; Discriminator Loss: 0.7120, Generator Loss: 0.9448\n",
      "Epoch Number: [72/250]; Batch 0/938; Discriminator Loss: 0.6070, Generator Loss: 1.3554\n",
      "Epoch Number: [73/250]; Batch 0/938; Discriminator Loss: 0.7384, Generator Loss: 1.1437\n",
      "Epoch Number: [74/250]; Batch 0/938; Discriminator Loss: 0.4951, Generator Loss: 1.3064\n",
      "Epoch Number: [75/250]; Batch 0/938; Discriminator Loss: 0.4818, Generator Loss: 1.4167\n",
      "Epoch Number: [76/250]; Batch 0/938; Discriminator Loss: 0.4713, Generator Loss: 1.2906\n",
      "Epoch Number: [77/250]; Batch 0/938; Discriminator Loss: 0.5043, Generator Loss: 1.1709\n",
      "Epoch Number: [78/250]; Batch 0/938; Discriminator Loss: 0.5820, Generator Loss: 1.3126\n",
      "Epoch Number: [79/250]; Batch 0/938; Discriminator Loss: 0.4974, Generator Loss: 1.2465\n",
      "Epoch Number: [80/250]; Batch 0/938; Discriminator Loss: 0.6475, Generator Loss: 0.9378\n",
      "Epoch Number: [81/250]; Batch 0/938; Discriminator Loss: 0.5732, Generator Loss: 1.3460\n",
      "Epoch Number: [82/250]; Batch 0/938; Discriminator Loss: 0.7387, Generator Loss: 1.2038\n",
      "Epoch Number: [83/250]; Batch 0/938; Discriminator Loss: 0.5715, Generator Loss: 1.2238\n",
      "Epoch Number: [84/250]; Batch 0/938; Discriminator Loss: 0.5102, Generator Loss: 1.3196\n",
      "Epoch Number: [85/250]; Batch 0/938; Discriminator Loss: 0.4770, Generator Loss: 1.4801\n",
      "Epoch Number: [86/250]; Batch 0/938; Discriminator Loss: 0.6333, Generator Loss: 1.3188\n",
      "Epoch Number: [87/250]; Batch 0/938; Discriminator Loss: 0.4576, Generator Loss: 1.2349\n",
      "Epoch Number: [88/250]; Batch 0/938; Discriminator Loss: 0.5369, Generator Loss: 1.2896\n",
      "Epoch Number: [89/250]; Batch 0/938; Discriminator Loss: 0.5803, Generator Loss: 1.2697\n",
      "Epoch Number: [90/250]; Batch 0/938; Discriminator Loss: 0.5141, Generator Loss: 1.2523\n",
      "Epoch Number: [91/250]; Batch 0/938; Discriminator Loss: 0.4847, Generator Loss: 1.2396\n",
      "Epoch Number: [92/250]; Batch 0/938; Discriminator Loss: 0.4803, Generator Loss: 1.3958\n",
      "Epoch Number: [93/250]; Batch 0/938; Discriminator Loss: 0.5456, Generator Loss: 1.2643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: [94/250]; Batch 0/938; Discriminator Loss: 0.4302, Generator Loss: 1.3008\n",
      "Epoch Number: [95/250]; Batch 0/938; Discriminator Loss: 0.5387, Generator Loss: 1.1230\n",
      "Epoch Number: [96/250]; Batch 0/938; Discriminator Loss: 0.5644, Generator Loss: 1.4317\n",
      "Epoch Number: [97/250]; Batch 0/938; Discriminator Loss: 0.4727, Generator Loss: 1.5491\n",
      "Epoch Number: [98/250]; Batch 0/938; Discriminator Loss: 0.5840, Generator Loss: 1.2995\n",
      "Epoch Number: [99/250]; Batch 0/938; Discriminator Loss: 0.6108, Generator Loss: 1.2319\n",
      "Epoch Number: [100/250]; Batch 0/938; Discriminator Loss: 0.5626, Generator Loss: 1.2219\n",
      "Epoch Number: [101/250]; Batch 0/938; Discriminator Loss: 0.6012, Generator Loss: 1.4123\n",
      "Epoch Number: [102/250]; Batch 0/938; Discriminator Loss: 0.5782, Generator Loss: 1.2345\n",
      "Epoch Number: [103/250]; Batch 0/938; Discriminator Loss: 0.5654, Generator Loss: 1.1988\n",
      "Epoch Number: [104/250]; Batch 0/938; Discriminator Loss: 0.4869, Generator Loss: 1.2649\n",
      "Epoch Number: [105/250]; Batch 0/938; Discriminator Loss: 0.5394, Generator Loss: 1.5030\n",
      "Epoch Number: [106/250]; Batch 0/938; Discriminator Loss: 0.4316, Generator Loss: 1.3802\n",
      "Epoch Number: [107/250]; Batch 0/938; Discriminator Loss: 0.4288, Generator Loss: 1.4016\n",
      "Epoch Number: [108/250]; Batch 0/938; Discriminator Loss: 0.4542, Generator Loss: 1.3307\n",
      "Epoch Number: [109/250]; Batch 0/938; Discriminator Loss: 0.4576, Generator Loss: 1.1923\n",
      "Epoch Number: [110/250]; Batch 0/938; Discriminator Loss: 0.5972, Generator Loss: 1.0703\n",
      "Epoch Number: [111/250]; Batch 0/938; Discriminator Loss: 0.4880, Generator Loss: 1.2762\n",
      "Epoch Number: [112/250]; Batch 0/938; Discriminator Loss: 0.5713, Generator Loss: 1.3888\n",
      "Epoch Number: [113/250]; Batch 0/938; Discriminator Loss: 0.5434, Generator Loss: 1.1059\n",
      "Epoch Number: [114/250]; Batch 0/938; Discriminator Loss: 0.6694, Generator Loss: 1.2070\n",
      "Epoch Number: [115/250]; Batch 0/938; Discriminator Loss: 0.4820, Generator Loss: 1.3949\n",
      "Epoch Number: [116/250]; Batch 0/938; Discriminator Loss: 0.5495, Generator Loss: 1.2333\n",
      "Epoch Number: [117/250]; Batch 0/938; Discriminator Loss: 0.4735, Generator Loss: 1.3989\n",
      "Epoch Number: [118/250]; Batch 0/938; Discriminator Loss: 0.6585, Generator Loss: 1.1177\n",
      "Epoch Number: [119/250]; Batch 0/938; Discriminator Loss: 0.5918, Generator Loss: 1.2286\n",
      "Epoch Number: [120/250]; Batch 0/938; Discriminator Loss: 0.5101, Generator Loss: 1.2260\n",
      "Epoch Number: [121/250]; Batch 0/938; Discriminator Loss: 0.4928, Generator Loss: 1.3936\n",
      "Epoch Number: [122/250]; Batch 0/938; Discriminator Loss: 0.5534, Generator Loss: 1.1566\n",
      "Epoch Number: [123/250]; Batch 0/938; Discriminator Loss: 0.4904, Generator Loss: 1.4040\n",
      "Epoch Number: [124/250]; Batch 0/938; Discriminator Loss: 0.5029, Generator Loss: 1.3100\n",
      "Epoch Number: [125/250]; Batch 0/938; Discriminator Loss: 0.5631, Generator Loss: 1.0998\n",
      "Epoch Number: [126/250]; Batch 0/938; Discriminator Loss: 0.5659, Generator Loss: 1.1032\n",
      "Epoch Number: [127/250]; Batch 0/938; Discriminator Loss: 0.5411, Generator Loss: 0.9752\n",
      "Epoch Number: [128/250]; Batch 0/938; Discriminator Loss: 0.5362, Generator Loss: 1.2379\n",
      "Epoch Number: [129/250]; Batch 0/938; Discriminator Loss: 0.5510, Generator Loss: 1.2945\n",
      "Epoch Number: [130/250]; Batch 0/938; Discriminator Loss: 0.5792, Generator Loss: 1.2387\n",
      "Epoch Number: [131/250]; Batch 0/938; Discriminator Loss: 0.4852, Generator Loss: 1.2805\n",
      "Epoch Number: [132/250]; Batch 0/938; Discriminator Loss: 0.4551, Generator Loss: 1.3800\n",
      "Epoch Number: [133/250]; Batch 0/938; Discriminator Loss: 0.5536, Generator Loss: 1.3646\n",
      "Epoch Number: [134/250]; Batch 0/938; Discriminator Loss: 0.4658, Generator Loss: 1.4988\n",
      "Epoch Number: [135/250]; Batch 0/938; Discriminator Loss: 0.3731, Generator Loss: 1.5911\n",
      "Epoch Number: [136/250]; Batch 0/938; Discriminator Loss: 0.4815, Generator Loss: 1.3603\n",
      "Epoch Number: [137/250]; Batch 0/938; Discriminator Loss: 0.4498, Generator Loss: 1.3411\n",
      "Epoch Number: [138/250]; Batch 0/938; Discriminator Loss: 0.4703, Generator Loss: 1.4091\n",
      "Epoch Number: [139/250]; Batch 0/938; Discriminator Loss: 0.5303, Generator Loss: 1.2095\n",
      "Epoch Number: [140/250]; Batch 0/938; Discriminator Loss: 0.5963, Generator Loss: 0.9672\n",
      "Epoch Number: [141/250]; Batch 0/938; Discriminator Loss: 0.4597, Generator Loss: 1.4780\n",
      "Epoch Number: [142/250]; Batch 0/938; Discriminator Loss: 0.5135, Generator Loss: 1.1550\n",
      "Epoch Number: [143/250]; Batch 0/938; Discriminator Loss: 0.5101, Generator Loss: 1.2478\n",
      "Epoch Number: [144/250]; Batch 0/938; Discriminator Loss: 0.5541, Generator Loss: 1.2509\n",
      "Epoch Number: [145/250]; Batch 0/938; Discriminator Loss: 0.5007, Generator Loss: 1.2803\n",
      "Epoch Number: [146/250]; Batch 0/938; Discriminator Loss: 0.4733, Generator Loss: 1.4985\n",
      "Epoch Number: [147/250]; Batch 0/938; Discriminator Loss: 0.5290, Generator Loss: 1.2355\n",
      "Epoch Number: [148/250]; Batch 0/938; Discriminator Loss: 0.4928, Generator Loss: 1.1291\n",
      "Epoch Number: [149/250]; Batch 0/938; Discriminator Loss: 0.4860, Generator Loss: 1.3237\n",
      "Epoch Number: [150/250]; Batch 0/938; Discriminator Loss: 0.5295, Generator Loss: 1.2746\n",
      "Epoch Number: [151/250]; Batch 0/938; Discriminator Loss: 0.5432, Generator Loss: 1.3149\n",
      "Epoch Number: [152/250]; Batch 0/938; Discriminator Loss: 0.4603, Generator Loss: 1.6268\n",
      "Epoch Number: [153/250]; Batch 0/938; Discriminator Loss: 0.4340, Generator Loss: 1.5876\n",
      "Epoch Number: [154/250]; Batch 0/938; Discriminator Loss: 0.4280, Generator Loss: 1.2569\n",
      "Epoch Number: [155/250]; Batch 0/938; Discriminator Loss: 0.5802, Generator Loss: 1.1064\n",
      "Epoch Number: [156/250]; Batch 0/938; Discriminator Loss: 0.4542, Generator Loss: 1.5895\n",
      "Epoch Number: [157/250]; Batch 0/938; Discriminator Loss: 0.4970, Generator Loss: 1.4219\n",
      "Epoch Number: [158/250]; Batch 0/938; Discriminator Loss: 0.5743, Generator Loss: 1.1629\n",
      "Epoch Number: [159/250]; Batch 0/938; Discriminator Loss: 0.5678, Generator Loss: 1.0918\n",
      "Epoch Number: [160/250]; Batch 0/938; Discriminator Loss: 0.4621, Generator Loss: 1.2959\n",
      "Epoch Number: [161/250]; Batch 0/938; Discriminator Loss: 0.5480, Generator Loss: 1.1881\n",
      "Epoch Number: [162/250]; Batch 0/938; Discriminator Loss: 0.5953, Generator Loss: 1.1743\n",
      "Epoch Number: [163/250]; Batch 0/938; Discriminator Loss: 0.4746, Generator Loss: 1.1691\n",
      "Epoch Number: [164/250]; Batch 0/938; Discriminator Loss: 0.5180, Generator Loss: 1.3660\n",
      "Epoch Number: [165/250]; Batch 0/938; Discriminator Loss: 0.4977, Generator Loss: 1.2148\n",
      "Epoch Number: [166/250]; Batch 0/938; Discriminator Loss: 0.5346, Generator Loss: 1.1407\n",
      "Epoch Number: [167/250]; Batch 0/938; Discriminator Loss: 0.4771, Generator Loss: 1.2182\n",
      "Epoch Number: [168/250]; Batch 0/938; Discriminator Loss: 0.5539, Generator Loss: 1.2834\n",
      "Epoch Number: [169/250]; Batch 0/938; Discriminator Loss: 0.4531, Generator Loss: 1.1875\n",
      "Epoch Number: [170/250]; Batch 0/938; Discriminator Loss: 0.5412, Generator Loss: 1.5642\n",
      "Epoch Number: [171/250]; Batch 0/938; Discriminator Loss: 0.6263, Generator Loss: 1.1434\n",
      "Epoch Number: [172/250]; Batch 0/938; Discriminator Loss: 0.5120, Generator Loss: 1.3861\n",
      "Epoch Number: [173/250]; Batch 0/938; Discriminator Loss: 0.4587, Generator Loss: 1.3404\n",
      "Epoch Number: [174/250]; Batch 0/938; Discriminator Loss: 0.5693, Generator Loss: 1.1930\n",
      "Epoch Number: [175/250]; Batch 0/938; Discriminator Loss: 0.4640, Generator Loss: 1.4351\n",
      "Epoch Number: [176/250]; Batch 0/938; Discriminator Loss: 0.4820, Generator Loss: 1.4164\n",
      "Epoch Number: [177/250]; Batch 0/938; Discriminator Loss: 0.4842, Generator Loss: 1.3344\n",
      "Epoch Number: [178/250]; Batch 0/938; Discriminator Loss: 0.5255, Generator Loss: 1.1802\n",
      "Epoch Number: [179/250]; Batch 0/938; Discriminator Loss: 0.4548, Generator Loss: 1.2908\n",
      "Epoch Number: [180/250]; Batch 0/938; Discriminator Loss: 0.5218, Generator Loss: 1.4581\n",
      "Epoch Number: [181/250]; Batch 0/938; Discriminator Loss: 0.5136, Generator Loss: 1.3374\n",
      "Epoch Number: [182/250]; Batch 0/938; Discriminator Loss: 0.5302, Generator Loss: 1.3168\n",
      "Epoch Number: [183/250]; Batch 0/938; Discriminator Loss: 0.6019, Generator Loss: 1.0149\n",
      "Epoch Number: [184/250]; Batch 0/938; Discriminator Loss: 0.4848, Generator Loss: 1.0556\n",
      "Epoch Number: [185/250]; Batch 0/938; Discriminator Loss: 0.4911, Generator Loss: 1.4407\n",
      "Epoch Number: [186/250]; Batch 0/938; Discriminator Loss: 0.4550, Generator Loss: 1.1470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: [187/250]; Batch 0/938; Discriminator Loss: 0.5007, Generator Loss: 1.1164\n",
      "Epoch Number: [188/250]; Batch 0/938; Discriminator Loss: 0.5355, Generator Loss: 1.3347\n",
      "Epoch Number: [189/250]; Batch 0/938; Discriminator Loss: 0.4781, Generator Loss: 1.3477\n",
      "Epoch Number: [190/250]; Batch 0/938; Discriminator Loss: 0.4309, Generator Loss: 1.3709\n",
      "Epoch Number: [191/250]; Batch 0/938; Discriminator Loss: 0.5156, Generator Loss: 1.1805\n",
      "Epoch Number: [192/250]; Batch 0/938; Discriminator Loss: 0.5224, Generator Loss: 1.3294\n",
      "Epoch Number: [193/250]; Batch 0/938; Discriminator Loss: 0.4985, Generator Loss: 1.3438\n",
      "Epoch Number: [194/250]; Batch 0/938; Discriminator Loss: 0.5541, Generator Loss: 1.0952\n",
      "Epoch Number: [195/250]; Batch 0/938; Discriminator Loss: 0.5166, Generator Loss: 1.4949\n",
      "Epoch Number: [196/250]; Batch 0/938; Discriminator Loss: 0.5257, Generator Loss: 1.2748\n",
      "Epoch Number: [197/250]; Batch 0/938; Discriminator Loss: 0.5532, Generator Loss: 1.3665\n",
      "Epoch Number: [198/250]; Batch 0/938; Discriminator Loss: 0.5032, Generator Loss: 1.4647\n",
      "Epoch Number: [199/250]; Batch 0/938; Discriminator Loss: 0.4682, Generator Loss: 1.4692\n",
      "Epoch Number: [200/250]; Batch 0/938; Discriminator Loss: 0.4817, Generator Loss: 1.4552\n",
      "Epoch Number: [201/250]; Batch 0/938; Discriminator Loss: 0.5709, Generator Loss: 1.3028\n",
      "Epoch Number: [202/250]; Batch 0/938; Discriminator Loss: 0.6486, Generator Loss: 1.1068\n",
      "Epoch Number: [203/250]; Batch 0/938; Discriminator Loss: 0.4994, Generator Loss: 1.3656\n",
      "Epoch Number: [204/250]; Batch 0/938; Discriminator Loss: 0.4496, Generator Loss: 1.2869\n",
      "Epoch Number: [205/250]; Batch 0/938; Discriminator Loss: 0.4711, Generator Loss: 1.4531\n",
      "Epoch Number: [206/250]; Batch 0/938; Discriminator Loss: 0.4820, Generator Loss: 1.3969\n",
      "Epoch Number: [207/250]; Batch 0/938; Discriminator Loss: 0.4917, Generator Loss: 1.3946\n",
      "Epoch Number: [208/250]; Batch 0/938; Discriminator Loss: 0.5906, Generator Loss: 1.0722\n",
      "Epoch Number: [209/250]; Batch 0/938; Discriminator Loss: 0.5491, Generator Loss: 1.3673\n",
      "Epoch Number: [210/250]; Batch 0/938; Discriminator Loss: 0.6504, Generator Loss: 1.0970\n",
      "Epoch Number: [211/250]; Batch 0/938; Discriminator Loss: 0.4896, Generator Loss: 1.2339\n",
      "Epoch Number: [212/250]; Batch 0/938; Discriminator Loss: 0.5751, Generator Loss: 1.1069\n",
      "Epoch Number: [213/250]; Batch 0/938; Discriminator Loss: 0.5530, Generator Loss: 1.3643\n",
      "Epoch Number: [214/250]; Batch 0/938; Discriminator Loss: 0.5500, Generator Loss: 1.3241\n",
      "Epoch Number: [215/250]; Batch 0/938; Discriminator Loss: 0.4981, Generator Loss: 1.4332\n",
      "Epoch Number: [216/250]; Batch 0/938; Discriminator Loss: 0.5892, Generator Loss: 1.1582\n",
      "Epoch Number: [217/250]; Batch 0/938; Discriminator Loss: 0.5178, Generator Loss: 1.2346\n",
      "Epoch Number: [218/250]; Batch 0/938; Discriminator Loss: 0.5098, Generator Loss: 1.1290\n",
      "Epoch Number: [219/250]; Batch 0/938; Discriminator Loss: 0.5860, Generator Loss: 1.0692\n",
      "Epoch Number: [220/250]; Batch 0/938; Discriminator Loss: 0.5467, Generator Loss: 1.3456\n",
      "Epoch Number: [221/250]; Batch 0/938; Discriminator Loss: 0.5936, Generator Loss: 1.1247\n",
      "Epoch Number: [222/250]; Batch 0/938; Discriminator Loss: 0.5003, Generator Loss: 1.3691\n",
      "Epoch Number: [223/250]; Batch 0/938; Discriminator Loss: 0.4592, Generator Loss: 1.2733\n",
      "Epoch Number: [224/250]; Batch 0/938; Discriminator Loss: 0.4891, Generator Loss: 1.0743\n",
      "Epoch Number: [225/250]; Batch 0/938; Discriminator Loss: 0.4402, Generator Loss: 1.5651\n",
      "Epoch Number: [226/250]; Batch 0/938; Discriminator Loss: 0.5295, Generator Loss: 1.2054\n",
      "Epoch Number: [227/250]; Batch 0/938; Discriminator Loss: 0.4786, Generator Loss: 1.2603\n",
      "Epoch Number: [228/250]; Batch 0/938; Discriminator Loss: 0.5397, Generator Loss: 1.0784\n",
      "Epoch Number: [229/250]; Batch 0/938; Discriminator Loss: 0.5868, Generator Loss: 1.0562\n",
      "Epoch Number: [230/250]; Batch 0/938; Discriminator Loss: 0.4705, Generator Loss: 1.2518\n",
      "Epoch Number: [231/250]; Batch 0/938; Discriminator Loss: 0.5516, Generator Loss: 1.1500\n",
      "Epoch Number: [232/250]; Batch 0/938; Discriminator Loss: 0.5715, Generator Loss: 1.0036\n",
      "Epoch Number: [233/250]; Batch 0/938; Discriminator Loss: 0.4806, Generator Loss: 1.5222\n",
      "Epoch Number: [234/250]; Batch 0/938; Discriminator Loss: 0.4952, Generator Loss: 1.1803\n",
      "Epoch Number: [235/250]; Batch 0/938; Discriminator Loss: 0.5034, Generator Loss: 1.3142\n",
      "Epoch Number: [236/250]; Batch 0/938; Discriminator Loss: 0.5007, Generator Loss: 1.2938\n",
      "Epoch Number: [237/250]; Batch 0/938; Discriminator Loss: 0.5012, Generator Loss: 1.1357\n",
      "Epoch Number: [238/250]; Batch 0/938; Discriminator Loss: 0.5395, Generator Loss: 1.0854\n",
      "Epoch Number: [239/250]; Batch 0/938; Discriminator Loss: 0.5777, Generator Loss: 1.2530\n",
      "Epoch Number: [240/250]; Batch 0/938; Discriminator Loss: 0.5919, Generator Loss: 1.1144\n",
      "Epoch Number: [241/250]; Batch 0/938; Discriminator Loss: 0.4742, Generator Loss: 1.2761\n",
      "Epoch Number: [242/250]; Batch 0/938; Discriminator Loss: 0.5817, Generator Loss: 1.3935\n",
      "Epoch Number: [243/250]; Batch 0/938; Discriminator Loss: 0.5273, Generator Loss: 1.2545\n",
      "Epoch Number: [244/250]; Batch 0/938; Discriminator Loss: 0.4667, Generator Loss: 1.3097\n",
      "Epoch Number: [245/250]; Batch 0/938; Discriminator Loss: 0.5758, Generator Loss: 1.1165\n",
      "Epoch Number: [246/250]; Batch 0/938; Discriminator Loss: 0.5472, Generator Loss: 1.2659\n",
      "Epoch Number: [247/250]; Batch 0/938; Discriminator Loss: 0.5152, Generator Loss: 1.2554\n",
      "Epoch Number: [248/250]; Batch 0/938; Discriminator Loss: 0.5444, Generator Loss: 1.1372\n",
      "Epoch Number: [249/250]; Batch 0/938; Discriminator Loss: 0.5403, Generator Loss: 1.0361\n"
     ]
    }
   ],
   "source": [
    "# Reset step count (tracked at end of loop)\n",
    "step = 0\n",
    "\n",
    "# Run training\n",
    "for epoch in range(epochs):\n",
    "    for i, (real, labels) in enumerate(loader):\n",
    "        real = real.view(-1, image_size).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "# Train Discriminator\n",
    "        gen_noise   = torch.randn((batch_size, z)).to(device)\n",
    "        generated   = g(gen_noise)\n",
    "    # Real images      \n",
    "        d_real      = d(real).view(-1)\n",
    "        d_real_loss = criterion(d_real, torch.ones_like(d_real))\n",
    "    # Fake images\n",
    "        d_gen_      = d(generated).view(-1)\n",
    "        d_gen__loss = criterion(d_gen_, torch.zeros_like(d_gen_))\n",
    "    # Calculate loss\n",
    "        D_loss      = (d_real_loss+d_gen__loss)/2\n",
    "    # Reset gradient, backpropagate, optimize\n",
    "        d.zero_grad()\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        d_optim.step()\n",
    "# Train Generator\n",
    "        output = d(generated).view(-1)\n",
    "        G_loss = criterion(output, torch.ones_like(output))\n",
    "    # Reset gradient, backpropagate, optimize\n",
    "        g.zero_grad()\n",
    "        G_loss.backward()\n",
    "        g_optim.step()\n",
    "        if i == 0:\n",
    "            print(f\"Epoch Number: [{epoch}/{epochs}]; Batch {i}/{len(loader)}; Discriminator Loss: {D_loss:.4f}, Generator Loss: {G_loss:.4f}\")\n",
    "            with torch.no_grad():\n",
    "                generated = g(set_noise).reshape(-1,1,28,28)\n",
    "                data = real.reshape(-1,1,28,28)\n",
    "                gen__img_grid = torchvision.utils.make_grid(generated, normalize = True)\n",
    "                real_img_grid = torchvision.utils.make_grid(data, normalize = True)\n",
    "                fake_imgs.add_image(\"MNIST generated images\", gen__img_grid, global_step=step)\n",
    "                real_imgs.add_image(\"MNIST real images\", real_img_grid, global_step=step)\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11352), started 0:29:50 ago. (Use '!kill 11352' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f6b251deb1ec6e08\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f6b251deb1ec6e08\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=\"C:/Users/Nick/data/GAN/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "2020-12-11 14:58:19.596244: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
       "2020-12-11 14:58:19.601198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "usage: tensorboard [-h] [--helpfull] {serve,dev} ...\n",
       "tensorboard: error: invalid choice: '!kill' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard !kill 12188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
